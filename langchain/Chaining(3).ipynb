{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2C2WkYTGotu",
        "outputId": "f66d9ff2-ad2f-454e-d06b-d8af7a00fafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/274.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m911.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "tensorflow 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install -U --upgrade --quiet langchain-google-vertexai langchain-google-genai langchain-community langchain unstructured lark chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-vuXNWuKBZH"
      },
      "source": [
        "Installing all necessary python modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6vvVhVLGtl_",
        "outputId": "e1b5492b-1bc7-458e-c8ae-672aef24419d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: langchain\n",
            "Version: 0.1.14\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-core\n",
            "Version: 0.1.37\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, requests, tenacity\n",
            "Required-by: langchain, langchain-community, langchain-google-genai, langchain-google-vertexai, langchain-text-splitters\n"
          ]
        }
      ],
      "source": [
        "!pip show langchain langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auzQuc96DZ9c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('KEY')\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E83k0vY_KF1R"
      },
      "source": [
        "Setting up the environment, connecting my API key to be able to access the Gemini through the Colab environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoMZVDSbgrEf"
      },
      "source": [
        "Want to get a list of models that I can use...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cy8ttDgAHnRa",
        "outputId": "72251099-ea62-4c79-c092-4e6d1dfdab82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "models = [m for m in genai.list_models()]\n",
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81dbXC8WKSfU"
      },
      "source": [
        "Testing that the API keys are in fact working and that I can access/test Gemini from within this environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S6rXkeVJsNH"
      },
      "outputs": [],
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "result = model.invoke(\"Waht is an LLM?\")\n",
        "\n",
        "Markdown(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1n60YFugyUu"
      },
      "source": [
        "Can begin actually working on the chaining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j68vXrWnZJ4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# Define the initial prompt template for generating project architecture\n",
        "architecture_prompt_template = \"\"\"Given the project description below, generate an initial architecture outline suitable for GCP using Terraform. Keep it concise and to the point.\n",
        "\n",
        "Project Description:\n",
        "{project_description}\n",
        "\n",
        "User Architecture Feedback:\n",
        "{user_architecture_feedback}\n",
        "\n",
        "Initial Architecture Outline:\"\"\"\n",
        "\n",
        "# Define the prompt template for generating Terraform code based on confirmed architecture\n",
        "terraform_prompt_template = \"\"\"\n",
        "Given the confirmed architecture outline below, generate the full Terraform code necessary to deploy the project on GCP. The generated code should reflect the unique requirements of the specified architecture, focusing on creating a scalable and secure infrastructure. Use the example below as a guide for the format and structure but develop your own resource configurations and names based on the project's needs. Your code should include a Compute Engine instance, appropriate networking setup, and any other resources deemed necessary for a basic web application.\n",
        "\n",
        "Note: The example is for illustration only. Please generate custom configurations based on the architecture outline.\n",
        "\n",
        "Example Terraform Code:\n",
        "resource \"google_compute_instance\" \"default\" {{\n",
        "  name         = \"example-instance\"\n",
        "  machine_type = \"e2-medium\"\n",
        "  zone         = \"us-central1-a\"\n",
        "\n",
        "  boot_disk {{\n",
        "    initialize_params {{\n",
        "      image = \"debian-cloud/debian-9\"\n",
        "    }}\n",
        "  }}\n",
        "\n",
        "  network_interface {{\n",
        "    network = \"default\"\n",
        "    access_config {{}}\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Confirmed Architecture Outline:\n",
        "{{confirmed_architecture}}\n",
        "\n",
        "Existing Terraform Code:\n",
        "{{existing_terraform_code}}\n",
        "\n",
        "User Feedback:\n",
        "{{user_terraform_feedback}}\n",
        "\n",
        "Your Terraform Code:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Custom prompt templates\n",
        "architecture_prompt = PromptTemplate.from_template(\n",
        "    architecture_prompt_template)\n",
        "terraform_code_prompt = PromptTemplate.from_template(\n",
        "    terraform_prompt_template)\n",
        "\n",
        "# Define the interaction chain for architecture generation and confirmation\n",
        "architecture_chain = ({\n",
        "    \"project_description\": RunnablePassthrough(),\n",
        "    \"user_architecture_feedback\": RunnablePassthrough()\n",
        "}\n",
        "    | architecture_prompt\n",
        "    | model\n",
        "    | StrOutputParser())\n",
        "\n",
        "# Define the interaction chain for Terraform code generation\n",
        "terraform_code_chain = ({\n",
        "    \"confirmed_architecture\": RunnablePassthrough(),\n",
        "    \"existing_terraform_code\": RunnablePassthrough(),\n",
        "    \"user_terraform_feedback\": RunnablePassthrough()\n",
        "}\n",
        "    | terraform_code_prompt\n",
        "    | model\n",
        "    | StrOutputParser())\n",
        "\n",
        "# Define the prompt template for suggesting improvements\n",
        "improvement_prompt_template = \"\"\"\n",
        "Given the confirmed architecture outline below, suggest potential improvements that could enhance the project's efficiency, scalability, or reliability.\n",
        "\n",
        "Confirmed Architecture Outline:\n",
        "{confirmed_architecture}\n",
        "\n",
        "Suggested Improvements:\"\"\"\n",
        "\n",
        "improvement_prompt = PromptTemplate.from_template(\n",
        "    improvement_prompt_template)\n",
        "\n",
        "# Define the interaction chain for improvement suggestions\n",
        "improvement_chain = ({\n",
        "    \"confirmed_architecture\": RunnablePassthrough()\n",
        "}\n",
        "    | improvement_prompt\n",
        "    | model\n",
        "    | StrOutputParser())\n",
        "\n",
        "project_description = input(\"Enter the project description: \")\n",
        "\n",
        "user_architecture_feedback = \"Initial Feedback (None)\"\n",
        "user_terraform_feedback = \"Initial Feedback (None)\"\n",
        "existing_terraform_code = \"\"\n",
        "\n",
        "architecture = None\n",
        "while not architecture:\n",
        "  try:\n",
        "    architecture = architecture_chain.invoke({\n",
        "        \"project_description\": project_description,\n",
        "        \"user_architecture_feedback\": \"user_architecture_feedback\"\n",
        "    })\n",
        "  except Exception as e:\n",
        "    print(\"Error:\", e)\n",
        "    break\n",
        "\n",
        "  print(\"Generated Architecture: \", architecture)\n",
        "  user_confirmation = input(\"User confirms this architecture? (yes/no): \")\n",
        "  if user_confirmation.lower() == 'yes':\n",
        "    break\n",
        "  else:\n",
        "    user_architecture_feedback = input(\"Please describe the issues you have with the architecture: \")\n",
        "\n",
        "improvements = improvement_chain.invoke(\n",
        "  {\"confirmed_architecture\": architecture})\n",
        "\n",
        "# Display the suggested improvements to the user\n",
        "print(\"Suggested Improvements: \", improvements)\n",
        "user_confirmation = input(\n",
        "  \"Would you like to incorporate suggested improvements? (yes/no): \")\n",
        "\n",
        "if user_confirmation.lower() == 'yes':\n",
        "    # Update the architecture with the improvements\n",
        "    architecture += \"\\n\\nIncorporated Improvements:\\n\" + improvements\n",
        "\n",
        "# print()\n",
        "# print(architecture)\n",
        "terraform_code_correct = False\n",
        "terraform_code_generated = None\n",
        "while terraform_code_correct == False:\n",
        "  print(\"Architecture: \", architecture)\n",
        "  try:\n",
        "    terraform_code_generated = terraform_code_chain.invoke({\n",
        "        \"confirmed_architecture\": architecture,\n",
        "        \"existing_terraform_code\": existing_terraform_code,\n",
        "        \"user_terraform_feedback\": user_terraform_feedback\n",
        "    })\n",
        "  except Exception as e:\n",
        "    print(\"Error:\", e)\n",
        "    break\n",
        "\n",
        "  print(\"Generated Terraform Code: \", terraform_code_generated)\n",
        "  user_confirmed_terraform_code = input(\"User confirms this Terraform code? (yes/no): \")\n",
        "  if user_confirmed_terraform_code.lower() == 'yes':\n",
        "    terraform_code_correct = True\n",
        "  else:\n",
        "      existing_terraform_code = terraform_code_generated\n",
        "      user_terraform_feedback = input(\"What is wrong with the code? \")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L_QRHC0X4LMx",
        "outputId": "0d2bbed1-764a-4d25-b3fd-2da094d18973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the project description: Generate me terraform code that will allow me to host a website on GCP\n",
            "Generated Architecture:  **Infrastructure**\n",
            "\n",
            "* **Virtual Machine:** Host the website's application and content.\n",
            "* **Cloud Storage:** Store website files and assets.\n",
            "* **Load Balancer:** Distribute traffic to the virtual machine.\n",
            "* **DNS Management:** Manage domain name records.\n",
            "\n",
            "**Networking**\n",
            "\n",
            "* **Virtual Private Cloud (VPC):** Isolate the website's network resources.\n",
            "* **Firewall Rules:** Control inbound and outbound traffic.\n",
            "\n",
            "**Management**\n",
            "\n",
            "* **Service Account:** Authenticate Terraform operations.\n",
            "* **IAM Roles:** Assign permissions for resource management.\n",
            "* **Monitoring:** Configure metrics and alerts for website performance.\n",
            "\n",
            "**Terraform Modules**\n",
            "\n",
            "* **Virtual Machine:** Create and configure the virtual machine.\n",
            "* **Cloud Storage:** Create and configure the Cloud Storage bucket.\n",
            "* **Load Balancer:** Create and configure the load balancer.\n",
            "* **DNS Management:** Manage DNS records for the website.\n",
            "* **Network:** Create and configure the VPC and firewall rules.\n",
            "* **Service Account:** Create and configure the service account.\n",
            "* **Monitoring:** Configure monitoring for the website.\n",
            "User confirms this architecture? (yes/no): yes\n",
            "Suggested Improvements:  **Efficiency**\n",
            "\n",
            "* **Use a content delivery network (CDN):** A CDN caches static content closer to users, reducing latency and improving page load times.\n",
            "* **Optimize the virtual machine configuration:** Right-size the virtual machine to meet the website's performance requirements, avoiding over or under-provisioning.\n",
            "* **Implement caching mechanisms:** Cache frequently accessed data to reduce database load and improve performance.\n",
            "\n",
            "**Scalability**\n",
            "\n",
            "* **Design for horizontal scaling:** Use multiple virtual machines behind the load balancer to handle increased traffic.\n",
            "* **Use autoscaling:** Automatically adjust the number of virtual machines based on website traffic patterns.\n",
            "* **Consider using a serverless architecture:** Serverless platforms automatically scale resources based on demand, eliminating the need for manual scaling.\n",
            "\n",
            "**Reliability**\n",
            "\n",
            "* **Implement redundancy:** Create redundant instances of critical components, such as the load balancer and virtual machines, to ensure availability in case of failure.\n",
            "* **Use a distributed database:** Distribute data across multiple nodes to increase fault tolerance and improve performance.\n",
            "* **Monitor and alert:** Implement robust monitoring and alerting systems to detect and respond to issues promptly.\n",
            "* **Implement backups and disaster recovery:** Regularly backup website data and have a disaster recovery plan in place to minimize downtime in case of catastrophic events.\n",
            "Would you like to incorporate suggested improvements? (yes/no): yes\n",
            "Architecture:  **Infrastructure**\n",
            "\n",
            "* **Virtual Machine:** Host the website's application and content.\n",
            "* **Cloud Storage:** Store website files and assets.\n",
            "* **Load Balancer:** Distribute traffic to the virtual machine.\n",
            "* **DNS Management:** Manage domain name records.\n",
            "\n",
            "**Networking**\n",
            "\n",
            "* **Virtual Private Cloud (VPC):** Isolate the website's network resources.\n",
            "* **Firewall Rules:** Control inbound and outbound traffic.\n",
            "\n",
            "**Management**\n",
            "\n",
            "* **Service Account:** Authenticate Terraform operations.\n",
            "* **IAM Roles:** Assign permissions for resource management.\n",
            "* **Monitoring:** Configure metrics and alerts for website performance.\n",
            "\n",
            "**Terraform Modules**\n",
            "\n",
            "* **Virtual Machine:** Create and configure the virtual machine.\n",
            "* **Cloud Storage:** Create and configure the Cloud Storage bucket.\n",
            "* **Load Balancer:** Create and configure the load balancer.\n",
            "* **DNS Management:** Manage DNS records for the website.\n",
            "* **Network:** Create and configure the VPC and firewall rules.\n",
            "* **Service Account:** Create and configure the service account.\n",
            "* **Monitoring:** Configure monitoring for the website.\n",
            "\n",
            "Incorporated Improvements:\n",
            "**Efficiency**\n",
            "\n",
            "* **Use a content delivery network (CDN):** A CDN caches static content closer to users, reducing latency and improving page load times.\n",
            "* **Optimize the virtual machine configuration:** Right-size the virtual machine to meet the website's performance requirements, avoiding over or under-provisioning.\n",
            "* **Implement caching mechanisms:** Cache frequently accessed data to reduce database load and improve performance.\n",
            "\n",
            "**Scalability**\n",
            "\n",
            "* **Design for horizontal scaling:** Use multiple virtual machines behind the load balancer to handle increased traffic.\n",
            "* **Use autoscaling:** Automatically adjust the number of virtual machines based on website traffic patterns.\n",
            "* **Consider using a serverless architecture:** Serverless platforms automatically scale resources based on demand, eliminating the need for manual scaling.\n",
            "\n",
            "**Reliability**\n",
            "\n",
            "* **Implement redundancy:** Create redundant instances of critical components, such as the load balancer and virtual machines, to ensure availability in case of failure.\n",
            "* **Use a distributed database:** Distribute data across multiple nodes to increase fault tolerance and improve performance.\n",
            "* **Monitor and alert:** Implement robust monitoring and alerting systems to detect and respond to issues promptly.\n",
            "* **Implement backups and disaster recovery:** Regularly backup website data and have a disaster recovery plan in place to minimize downtime in case of catastrophic events.\n",
            "Generated Terraform Code:  **Confirmed Architecture Outline:**\n",
            "\n",
            "- **Compute Engine:**\n",
            "  - 2 instances (e2-medium) for web servers\n",
            "  - 1 instance (e2-small) for database\n",
            "- **Networking:**\n",
            "  - VPC with private and public subnets\n",
            "  - Internal load balancer for web servers\n",
            "  - External load balancer for public access\n",
            "- **Databases:**\n",
            "  - Cloud SQL for MySQL database (2 GB)\n",
            "- **Storage:**\n",
            "  - Cloud Storage bucket for static files\n",
            "- **Security:**\n",
            "  - Firewall rules for controlled access\n",
            "  - SSL certificate for the external load balancer\n",
            "\n",
            "**Revised Terraform Code:**\n",
            "\n",
            "```\n",
            "# VPC and Subnets\n",
            "resource \"google_compute_network\" \"vpc\" {\n",
            "  name = \"my-vpc\"\n",
            "}\n",
            "\n",
            "resource \"google_compute_subnetwork\" \"private-subnet\" {\n",
            "  name          = \"private-subnet\"\n",
            "  network       = google_compute_network.vpc.name\n",
            "  ip_cidr_range = \"10.0.0.0/24\"\n",
            "}\n",
            "\n",
            "resource \"google_compute_subnetwork\" \"public-subnet\" {\n",
            "  name          = \"public-subnet\"\n",
            "  network       = google_compute_network.vpc.name\n",
            "  ip_cidr_range = \"10.1.0.0/24\"\n",
            "}\n",
            "\n",
            "# Firewall Rules\n",
            "resource \"google_compute_firewall\" \"internal-web-servers\" {\n",
            "  name        = \"internal-web-servers\"\n",
            "  direction   = \"INGRESS\"\n",
            "  priority    = 100\n",
            "  source_ranges = [\"10.0.0.0/24\"]\n",
            "  target_tags  = [\"web-server\"]\n",
            "  allowed {\n",
            "    protocol = \"tcp\"\n",
            "    ports    = [\"80\", \"443\"]\n",
            "  }\n",
            "}\n",
            "\n",
            "resource \"google_compute_firewall\" \"external-web-servers\" {\n",
            "  name        = \"external-web-servers\"\n",
            "  direction   = \"INGRESS\"\n",
            "  priority    = 200\n",
            "  source_ranges = [\"0.0.0.0/0\"]\n",
            "  target_tags  = [\"web-server\"]\n",
            "  allowed {\n",
            "    protocol = \"tcp\"\n",
            "    ports    = [\"80\", \"443\"]\n",
            "  }\n",
            "}\n",
            "\n",
            "# Internal Load Balancer\n",
            "resource \"google_compute_backend_service\" \"web-servers\" {\n",
            "  name        = \"web-servers\"\n",
            "  protocol    = \"HTTP\"\n",
            "  port_name   = \"web-port\"\n",
            "  health_checks = [\"health-check\"]\n",
            "}\n",
            "\n",
            "resource \"google_compute_health_check\" \"health-check\" {\n",
            "  name   = \"health-check\"\n",
            "  type   = \"HTTP\"\n",
            "  port   = 80\n",
            "  request_path = \"/\"\n",
            "}\n",
            "\n",
            "resource \"google_compute_url_map\" \"web-map\" {\n",
            "  name = \"web-map\"\n",
            "  default_service = google_compute_backend_service.web-servers.name\n",
            "}\n",
            "\n",
            "resource \"google_compute_target_http_proxy\" \"web-proxy\" {\n",
            "  name = \"web-proxy\"\n",
            "  url_map = google_compute_url_map.web-map.name\n",
            "}\n",
            "\n",
            "resource \"google_compute_forwarding_rule\" \"internal-web-forwarding\" {\n",
            "  name        = \"internal-web-forwarding\"\n",
            "  ip_protocol = \"TCP\"\n",
            "  port_range  = \"8080\"\n",
            "  target      = google_compute_target_http_proxy.web-proxy.name\n",
            "  subnetwork  = google_compute_subnetwork.private-subnet.name\n",
            "}\n",
            "\n",
            "# External Load Balancer\n",
            "resource \"google_compute_backend_service\" \"public-web-servers\" {\n",
            "  name        = \"public-web-servers\"\n",
            "  protocol    = \"HTTP\"\n",
            "  port_name   = \"web-port\"\n",
            "  health_checks = [\"public-health-check\"]\n",
            "}\n",
            "\n",
            "resource \"google_compute_health_check\" \"public-health-check\" {\n",
            "  name   = \"public-health-check\"\n",
            "  type   = \"HTTP\"\n",
            "  port   = 80\n",
            "  request_path = \"/\"\n",
            "}\n",
            "\n",
            "resource \"google_compute_url_map\" \"public-web-map\" {\n",
            "  name = \"public-web-map\"\n",
            "  default_service = google_compute_backend_service.public-web-servers.name\n",
            "}\n",
            "\n",
            "resource \"google_compute_target_http_proxy\" \"public-web-proxy\" {\n",
            "  name = \"public-web-proxy\"\n",
            "  url_map = google_compute_url_map.public-web-map.name\n",
            "}\n",
            "\n",
            "resource \"google_compute_forwarding_rule\" \"external-web-forwarding\" {\n",
            "  name        = \"external-web-forwarding\"\n",
            "  ip_protocol = \"TCP\"\n",
            "  port_range  = \"80\"\n",
            "  target      = google_compute_target_http_proxy.public-web-proxy.name\n",
            "  subnetwork  = google_compute_subnetwork.public-subnet.name\n",
            "}\n",
            "\n",
            "# Compute Instances\n",
            "resource \"google_compute_instance\" \"web-server-1\" {\n",
            "  name         = \"web-server-1\"\n",
            "  machine_type = \"e2-medium\"\n",
            "  zone         = \"us-central1-a\"\n",
            "  network_interface {\n",
            "    network = google_compute_network.vpc.name\n",
            "    subnetwork = google_compute_subnetwork.private-subnet.name\n",
            "    access_config {}\n",
            "  }\n",
            "  tags = [\"web-server\"]\n",
            "  metadata = {\n",
            "    startup-script = \"sudo apt update && sudo apt install nginx && sudo service nginx start\"\n",
            "  }\n",
            "}\n",
            "\n",
            "resource \"google_compute_instance\" \"web-server-2\" {\n",
            "  name         = \"web-server-2\"\n",
            "  machine_type = \"e2-medium\"\n",
            "  zone         = \"us-central1-a\"\n",
            "  network_interface {\n",
            "    network = google_compute_network.vpc.name\n",
            "    subnetwork = google_compute_subnetwork.private-subnet.name\n",
            "    access_config {}\n",
            "  }\n",
            "  tags = [\"web-server\"]\n",
            "  metadata = {\n",
            "    startup-script = \"sudo apt update && sudo apt install nginx && sudo service nginx start\"\n",
            "  }\n",
            "}\n",
            "\n",
            "resource \"google_compute_instance\" \"database\" {\n",
            "  name         = \"database\"\n",
            "  machine_type = \"e2-small\"\n",
            "  zone         = \"us-central1-a\"\n",
            "  network_interface {\n",
            "    network = google_compute_network.vpc.name\n",
            "    subnetwork = google_compute_subnetwork.private-subnet.name\n",
            "    access_config {}\n",
            "  }\n",
            "  metadata = {\n",
            "    startup-script = \"sudo apt update && sudo apt install mysql-server && sudo service mysql start\"\n",
            "  }\n",
            "}\n",
            "\n",
            "# Cloud SQL Database\n",
            "resource \"google_sql_database\" \"my-database\" {\n",
            "  name     = \"my-database\"\n",
            "  instance = \"my-instance\"\n",
            "  database = \"my-database\"\n",
            "}\n",
            "\n",
            "# Cloud Storage Bucket\n",
            "resource \"google_storage_bucket\" \"my-bucket\" {\n",
            "  name          = \"my-bucket\"\n",
            "  location      = \"US\"\n",
            "  force_destroy = true\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-a0f9e6188927>\u001b[0m in \u001b[0;36m<cell line: 140>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Terraform Code: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterraform_code_generated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m   \u001b[0muser_confirmed_terraform_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User confirms this Terraform code? (yes/no): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muser_confirmed_terraform_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'yes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mterraform_code_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLb3ffYuKWLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vMopgYXkFzic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EbknwwZ8s9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}