{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --upgrade --quiet langchain-google-vertexai langchain-google-genai langchain-community langchain unstructured lark chromadb"
      ],
      "metadata": {
        "id": "e2C2WkYTGotu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing all necessary python modules"
      ],
      "metadata": {
        "id": "S-vuXNWuKBZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain langchain-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6vvVhVLGtl_",
        "outputId": "92dbe9fe-8ace-44fa-e31b-f70cb0a91a16"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.1.10\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-experimental\n",
            "---\n",
            "Name: langchain-core\n",
            "Version: 0.1.28\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, jsonpatch, langsmith, packaging, pydantic, PyYAML, requests, tenacity\n",
            "Required-by: langchain, langchain-community, langchain-experimental, langchain-google-genai, langchain-google-vertexai, langchain-openai, langchain-text-splitters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "auzQuc96DZ9c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('KEY')\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the environment, connecting my API key to be able to access the Gemini through the Colab environment"
      ],
      "metadata": {
        "id": "E83k0vY_KF1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Want to get a list of models that I can use...."
      ],
      "metadata": {
        "id": "DoMZVDSbgrEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [m for m in genai.list_models()]\n",
        "models"
      ],
      "metadata": {
        "id": "Cy8ttDgAHnRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3931178-e7a0-4b4d-e175-049d76630d34"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing that the API keys are in fact working and that I can access/test Gemini from within this environment"
      ],
      "metadata": {
        "id": "81dbXC8WKSfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "result = model.invoke(\"Waht is an LLM?\")\n",
        "\n",
        "Markdown(result.content)"
      ],
      "metadata": {
        "id": "6S6rXkeVJsNH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "d248a032-a683-481a-8d97-248cc16d4b70"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LLM stands for Large Language Model.\n\nLLMs are a type of neural network that has been trained on a massive amount of text data. This training allows them to understand and generate human-like text. LLMs are used in a variety of natural language processing (NLP) applications, such as:\n\n* Machine translation\n* Text summarization\n* Question answering\n* Chatbots\n* Language modeling\n\nLLMs are still under development, but they have already shown great promise for a variety of NLP tasks. As they continue to improve, they are likely to play an increasingly important role in our lives."
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can begin actually working on the chaining"
      ],
      "metadata": {
        "id": "G1n60YFugyUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6j68vXrWnZJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# Define the initial prompt template for generating project architecture\n",
        "architecture_prompt_template = \"\"\"Given the project description below, generate an initial architecture outline suitable for GCP using Terraform. Keep it concise and to the point.\n",
        "\n",
        "Project Description:\n",
        "{project_description}\n",
        "\n",
        "User Architecture Feedback:\n",
        "{user_architecture_feedback}\n",
        "\n",
        "Initial Architecture Outline:\"\"\"\n",
        "\n",
        "# Define the prompt template for generating Terraform code based on confirmed architecture\n",
        "terraform_prompt_template = \"\"\"\n",
        "Given the confirmed architecture outline below, generate the full Terraform code necessary to deploy the project on GCP. If you are asked for a revision, please make minimal changes to the existing Terraform code to accommodate the new request. Do not generate completely new code based on the asked revision. Try to find the most optimal way to add the new request while still maintaining the existing code structure.\n",
        "\n",
        "Confirmed Architecture Outline:\n",
        "{confirmed_architecture}\n",
        "\n",
        "Existing Terraform Code:\n",
        "{existing_terraform_code}\n",
        "\n",
        "User Feedback:\n",
        "{user_terraform_feedback}\n",
        "\n",
        "Revised Terraform Code:\"\"\"\n",
        "\n",
        "\n",
        "# Custom prompt templates\n",
        "architecture_prompt = PromptTemplate.from_template(architecture_prompt_template)\n",
        "terraform_code_prompt = PromptTemplate.from_template(terraform_prompt_template)\n",
        "\n",
        "# Define the interaction chain for architecture generation and confirmation\n",
        "architecture_chain = (\n",
        "    {\"project_description\": RunnablePassthrough(), \"user_architecture_feedback\": RunnablePassthrough()}\n",
        "    | architecture_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Define the interaction chain for Terraform code generation\n",
        "terraform_code_chain = (\n",
        "    {\"confirmed_architecture\": RunnablePassthrough(), \"existing_terraform_code\": RunnablePassthrough(), \"user_terraform_feedback\": RunnablePassthrough()}\n",
        "    | terraform_code_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# Get the project description from the user\n",
        "project_description = input(\"Enter the project description: \")\n",
        "# Define the prompt template for suggesting improvements\n",
        "improvement_prompt_template = \"\"\"\n",
        "Given the confirmed architecture outline below, suggest potential improvements that could enhance the project's efficiency, scalability, or reliability.\n",
        "\n",
        "Confirmed Architecture Outline:\n",
        "{confirmed_architecture}\n",
        "\n",
        "Suggested Improvements:\"\"\"\n",
        "\n",
        "improvement_prompt = PromptTemplate.from_template(improvement_prompt_template)\n",
        "\n",
        "# Define the interaction chain for improvement suggestions\n",
        "improvement_chain = (\n",
        "    {\"confirmed_architecture\": RunnablePassthrough()}\n",
        "    | improvement_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Get the project description from the user\n",
        "project_description = input(\"Enter the project description: \")\n",
        "\n",
        "user_architecture_feedback = \"\" # Initializing user feedback\n",
        "user_terraform_feedback = \"\" # Initializing user feedback\n",
        "existing_terraform_code = \"\" # Initializing existing Terraform code\n",
        "\n",
        "while True:\n",
        "    # Run the architecture_chain\n",
        "    architecture = architecture_chain.invoke({\"project_description\": project_description, \"user_architecture_feedback\": user_architecture_feedback})\n",
        "\n",
        "    # Display the generated architecture to the user\n",
        "    print(\"Generated Architecture: \", architecture)\n",
        "\n",
        "    # Ask for user confirmation\n",
        "    user_confirmation = input(\"User confirms this architecture? (yes/no): \")\n",
        "\n",
        "    # If user confirms, break the loop\n",
        "    if user_confirmation.lower() == 'yes':\n",
        "        break\n",
        "    else:\n",
        "      user_architecture_feedback = input(\"Please describe the issues you have with the architecture: \")\n",
        "\n",
        "# After user confirmation, run the improvement_chain\n",
        "improvements = improvement_chain.invoke({\"confirmed_architecture\": architecture})\n",
        "\n",
        "# Display the suggested improvements to the user\n",
        "print(\"Suggested Improvements: \", improvements)\n",
        "\n",
        "# Ask for user confirmation\n",
        "user_confirmation = input(\"Would you like to incorporate these improvements? (yes/no): \")\n",
        "\n",
        "# If user confirms, update the architecture\n",
        "if user_confirmation.lower() == 'yes':\n",
        "    architecture += \"\\n\\nIncorporated Improvements:\\n\" + improvements\n",
        "\n",
        "# After user confirmation, run the terraform_code_chain\n",
        "while True:\n",
        "    terraform_code = terraform_code_chain.invoke({\"confirmed_architecture\": architecture, \"existing_terraform_code\": existing_terraform_code, \"user_terraform_feedback\": user_terraform_feedback})\n",
        "    print(\"Generated Terraform Code: \", terraform_code)\n",
        "\n",
        "    # Ask for user confirmation\n",
        "    user_confirmation = input(\"User confirms this Terraform code? (yes/no): \")\n",
        "\n",
        "    # If user confirms, break the loop\n",
        "    if user_confirmation.lower() == 'yes':\n",
        "        existing_terraform_code = terraform_code\n",
        "        break\n",
        "    else:\n",
        "      user_terraform_feedback = input(\"Please describe the issues you have with the Terraform code: \")\n",
        "\n",
        "# Save the final Terraform code to a .hcl file\n",
        "with open('terraform_code.hcl', 'w') as file:\n",
        "    file.write(terraform_code)"
      ],
      "metadata": {
        "id": "jG17YCIDKYrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9de47a-307b-437f-9b27-755cf6eb00e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Architecture:  **Web Tier:**\n",
            "* Cloud Functions (for API endpoints)\n",
            "* Cloud CDN (for static content)\n",
            "\n",
            "**Application Tier:**\n",
            "* App Engine (for application code)\n",
            "\n",
            "**Data Tier:**\n",
            "* Cloud Spanner (for relational data)\n",
            "* Cloud Bigtable (for NoSQL data)\n",
            "\n",
            "**Infrastructure:**\n",
            "* Terraform (for provisioning and management)\n",
            "* Cloud IAM (for access control)\n",
            "* Cloud Logging and Monitoring (for observability)\n",
            "Suggested Improvements:  **Efficiency Enhancements:**\n",
            "\n",
            "* **Implement caching:** Leverage in-memory or distributed caching mechanisms (e.g., Memcache, Redis) to reduce database queries and improve response times.\n",
            "* **Optimize database queries:** Perform indexing, partitioning, and optimizing query parameters to minimize data retrieval latency.\n",
            "* **Consider serverless databases:** Explore managed database services like Cloud Firestore or Cloud Spanner for automatic scaling and reduced operational overhead.\n",
            "\n",
            "**Scalability Enhancements:**\n",
            "\n",
            "* **Implement autoscaling:** Configure Cloud Functions and App Engine to automatically scale based on load, ensuring optimal performance during peak traffic.\n",
            "* **Utilize load balancing:** Employ load balancers like Google Cloud Load Balancing to distribute traffic across multiple instances, increasing capacity and improving availability.\n",
            "* **Consider regional deployment:** Deploy application components in multiple regions to improve latency and reduce the impact of regional outages.\n",
            "\n",
            "**Reliability Enhancements:**\n",
            "\n",
            "* **Implement redundancy:** Replicate data across multiple Cloud Spanner instances or Bigtable clusters to protect against data loss in case of a single instance or cluster failure.\n",
            "* **Use a disaster recovery plan:** Establish a comprehensive disaster recovery strategy that defines the steps to recover the system in case of a major outage.\n",
            "* **Enhance observability:** Implement additional logging and monitoring tools to track key metrics, identify issues early, and facilitate proactive maintenance.\n",
            "\n",
            "**Additional Considerations:**\n",
            "\n",
            "* **Review resource utilization:** Monitor and analyze resource usage to identify areas for optimization, such as reducing CPU or memory consumption.\n",
            "* **Consider using a service mesh:** Implement a service mesh like Istio to manage communication between microservices, enhance resilience, and improve observability.\n",
            "* **Employ continuous integration and deployment (CI/CD):** Automate the build, test, and deployment process to ensure consistent and reliable software delivery.\n",
            "Generated Terraform Code:  ```\n",
            "# Define the Cloud Functions resource\n",
            "resource \"google_cloud_functions_function\" \"api_endpoints\" {\n",
            "  name      = \"api-endpoints\"\n",
            "  runtime   = \"nodejs10\"\n",
            "  entry_point = \"helloHttp\"\n",
            "  timeout = \"300s\"\n",
            "  available_memory_mb = \"256\"\n",
            "  source_archive_url = \"gs://my-source-bucket/functions-source-code.zip\"\n",
            "}\n",
            "\n",
            "# Define the Cloud CDN resource\n",
            "resource \"google_cloud_cdn_url_map\" \"static_content\" {\n",
            "  name = \"static-content-url-map\"\n",
            "}\n",
            "\n",
            "# Define the App Engine resource\n",
            "resource \"google_app_engine_application\" \"app_engine\" {\n",
            "  name = \"my-app\"\n",
            "  location = \"us-central1\"\n",
            "}\n",
            "\n",
            "# Define the Cloud Spanner resource\n",
            "resource \"google_cloud_spanner_instance\" \"cloud_spanner\" {\n",
            "  name = \"my-spanner-instance\"\n",
            "  config = \"regional-us-central1\"\n",
            "  nodes = 2\n",
            "  labels = {\n",
            "    env = \"production\"\n",
            "  }\n",
            "}\n",
            "\n",
            "# Define the Cloud Bigtable resource\n",
            "resource \"google_cloud_bigtable_instance\" \"cloud_bigtable\" {\n",
            "  name = \"my-bigtable-instance\"\n",
            "  display_name = \"My Bigtable Instance\"\n",
            "  type = \"PRODUCTION\"\n",
            "  labels = {\n",
            "    env = \"production\"\n",
            "  }\n",
            "}\n",
            "\n",
            "# Define the Terraform resource\n",
            "resource \"google_client_config\" \"terraform\" {\n",
            "  client_config = {\n",
            "    credentials = \"service_account_file_path\"\n",
            "  }\n",
            "}\n",
            "\n",
            "# Define the Cloud IAM resource\n",
            "resource \"google_iam_policy\" \"cloud_iam\" {\n",
            "  bindings = {\n",
            "    members = [\"user:owner@example.com\"]\n",
            "    role    = \"roles/owner\"\n",
            "  }\n",
            "}\n",
            "\n",
            "# Define the Cloud Logging and Monitoring resource\n",
            "resource \"google_logging_metric\" \"cloud_logging_and_monitoring\" {\n",
            "  name = \"my-log-metric\"\n",
            "  filter = \"logName:syslog\"\n",
            "  metric = {\n",
            "    type   = \"custom.googleapis.com/my_metric\"\n",
            "    labels = {\n",
            "      log_name = \"syslog\"\n",
            "    }\n",
            "  }\n",
            "  create_descriptor = true\n",
            "}\n",
            "```\n",
            "Generated Terraform Code:  ```\n",
            "# Define the Compute Engine instance\n",
            "resource \"google_compute_instance\" \"default\" {\n",
            "  name         = \"my-instance\"\n",
            "  machine_type = \"n1-standard-1\"\n",
            "  zone         = \"us-central1-a\"\n",
            "\n",
            "  boot_disk {\n",
            "    initialize_params {\n",
            "      image = \"debian-cloud/debian-11\"\n",
            "    }\n",
            "  }\n",
            "\n",
            "  network_interface {\n",
            "    name = \"global/networks/default\"\n",
            "  }\n",
            "}\n",
            "```\n",
            "Generated Terraform Code:  This context does not mention anything about Terraform code, so I cannot extract the requested data from the provided context.\n",
            "Generated Terraform Code:  Since there is no existing Terraform code or a revision provided, I cannot generate the requested Terraform code or make any revisions to it.\n",
            "Generated Terraform Code:  ```\n",
            "resource \"google_compute_instance\" \"simple_web_server\" {\n",
            "  name         = \"simple-web-server\"\n",
            "  machine_type = \"e2-standard-2\"\n",
            "  zone         = \"us-central1-a\"\n",
            "\n",
            "  boot_disk {\n",
            "    initialize_params {\n",
            "      image = \"projects/debian-cloud/global/images/family/debian-11\"\n",
            "    }\n",
            "  }\n",
            "\n",
            "  network_interface {\n",
            "    network = \"default\"\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}