{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e2C2WkYTGotu"
      },
      "outputs": [],
      "source": [
        "!pip install -U --upgrade --quiet langchain-google-vertexai langchain-google-genai langchain-community langchain unstructured lark chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-vuXNWuKBZH"
      },
      "source": [
        "Installing all necessary python modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6vvVhVLGtl_",
        "outputId": "e8c84540-5166-4356-d846-d85f22871fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.1.12\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-core\n",
            "Version: 0.1.32\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, jsonpatch, langsmith, packaging, pydantic, PyYAML, requests, tenacity\n",
            "Required-by: langchain, langchain-community, langchain-google-genai, langchain-google-vertexai, langchain-text-splitters\n"
          ]
        }
      ],
      "source": [
        "!pip show langchain langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auzQuc96DZ9c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('KEY')\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E83k0vY_KF1R"
      },
      "source": [
        "Setting up the environment, connecting my API key to be able to access the Gemini through the Colab environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoMZVDSbgrEf"
      },
      "source": [
        "Want to get a list of models that I can use...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cy8ttDgAHnRa",
        "outputId": "efd862f8-d7f4-4182-dc43-a2ca26db0418"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=1),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "models = [m for m in genai.list_models()]\n",
        "models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81dbXC8WKSfU"
      },
      "source": [
        "Testing that the API keys are in fact working and that I can access/test Gemini from within this environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6S6rXkeVJsNH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "741d3405-f916-4ec0-e045-8cb5c51a8fdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Large Language Model (LLM)"
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "result = model.invoke(\"Waht is an LLM?\")\n",
        "\n",
        "Markdown(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1n60YFugyUu"
      },
      "source": [
        "Can begin actually working on the chaining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j68vXrWnZJ4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jG17YCIDKYrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "695985f8-7ebf-4244-ae80-7f73af880f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the project description: I want you to create me a website\n",
            "Generated Architecture:  **Infrastructure:**\n",
            "\n",
            "* **Compute Engine:** Host web application and database\n",
            "* **Cloud Storage:** Store website content, backups, and logs\n",
            "* **Cloud Networking:** Provide network connectivity and security\n",
            "* **Load Balancer:** Distribute traffic across web instances\n",
            "\n",
            "**Services:**\n",
            "\n",
            "* **Cloud SQL:** Managed MySQL database\n",
            "* **Cloud DNS:** Manage DNS records for website\n",
            "* **Cloud CDN:** Improve website performance by caching static content\n",
            "* **Cloud Monitoring:** Monitor website and infrastructure health\n",
            "\n",
            "**Tools:**\n",
            "\n",
            "* **Terraform:** Manage and provision cloud resources\n",
            "* **CI/CD Pipeline:** Automate infrastructure deployment and updates\n",
            "User confirms this architecture? (yes/no): yes\n",
            "Suggested Improvements:  **Efficiency:**\n",
            "\n",
            "* **Consider using autoscaling:** Automatically adjust the number of Compute Engine instances based on traffic load to optimize resource utilization and cost.\n",
            "* **Implement caching mechanisms:** Use in-memory caching or a caching service like Memcached to reduce database load and improve performance.\n",
            "* **Optimize database queries:** Use indexing, query optimization techniques, and database tuning to reduce database query times.\n",
            "\n",
            "**Scalability:**\n",
            "\n",
            "* **Design for horizontal scaling:** Structure the application to easily add or remove web instances as traffic demands change.\n",
            "* **Use a distributed database:** Consider using a distributed database like Cloud Spanner or BigQuery to handle large data volumes and support high concurrency.\n",
            "* **Implement load balancing strategies:** Use advanced load balancing techniques like round-robin, weighted, or geo-based load balancing to distribute traffic optimally.\n",
            "\n",
            "**Reliability:**\n",
            "\n",
            "* **Implement redundancy:** Replicate critical components like the database and web application across multiple zones or regions to ensure high availability.\n",
            "* **Use a disaster recovery plan:** Establish a plan for recovering from catastrophic events, such as data loss or hardware failures.\n",
            "* **Monitor and alert proactively:** Set up monitoring alerts to detect potential issues early and trigger automated responses or notifications.\n",
            "Would you like to incorporate these improvements? (yes/no): yes\n",
            "Generated Terraform Code:  ```\n",
            "# Define the Compute Engine instance for the web application\n",
            "resource \"google_compute_instance\" \"web\" {\n",
            "  name         = \"web-instance\"\n",
            "  machine_type = \"e2-standard-4\"\n",
            "  zone         = \"us-central1-a\"\n",
            "  network_interface {\n",
            "    network = \"default\"\n",
            "  }\n",
            "  # Enable autoscaling for the web instance\n",
            "  autoscaler {\n",
            "    min_num_replicas = 1\n",
            "    max_num_replicas = 5\n",
            "    target_cpu_utilization_percent = 50\n",
            "  }\n",
            "}\n",
            "\n",
            "# Define the Cloud SQL instance for the database\n",
            "resource \"google_sql_database\" \"db\" {\n",
            "  name    = \"db-instance\"\n",
            "  database = \"mydb\"\n",
            "  charset = \"utf8\"\n",
            "}\n",
            "\n",
            "# Define the Cloud Storage bucket for website content, backups, and logs\n",
            "resource \"google_storage_bucket\" \"bucket\" {\n",
            "  name          = \"my-bucket\"\n",
            "  storage_class = \"COLDLINE\"\n",
            "}\n",
            "\n",
            "# Define the Cloud CDN instance for improved website performance\n",
            "resource \"google_cloud_cdn_instance\" \"cdn\" {\n",
            "  name = \"my-cdn\"\n",
            "}\n",
            "\n",
            "# Define the Cloud Monitoring instance for monitoring website and infrastructure health\n",
            "resource \"google_monitoring_uptime_check_config\" \"uptime_check\" {\n",
            "  display_name = \"my-uptime-check\"\n",
            "  monitored_resource {\n",
            "    type = \"uptime_url\"\n",
            "    labels = {\n",
            "      host = \"my-website.com\"\n",
            "    }\n",
            "  }\n",
            "  http_check {\n",
            "    path = \"/\"\n",
            "    port = 80\n",
            "  }\n",
            "}\n",
            "```\n",
            "User confirms this Terraform code? (yes/no): no\n",
            "Please describe the issues you have with the Terraform code: I also want VPN functionality\n",
            "Generated Terraform Code:  ```\n",
            "# Configure the VPN Gateway\n",
            "resource \"google_compute_vpn_gateway\" \"vpn_gateway\" {\n",
            "  name    = \"vpn-gateway\"\n",
            "  network = google_compute_network.default.name\n",
            "  region  = var.region\n",
            "}\n",
            "\n",
            "# Configure the VPN Tunnel\n",
            "resource \"google_compute_vpn_tunnel\" \"vpn_tunnel\" {\n",
            "  name                 = \"vpn-tunnel\"\n",
            "  peer_ip              = \"10.128.0.1\"\n",
            "  region               = var.region\n",
            "  target_vpn_gateway   = google_compute_vpn_gateway.vpn_gateway.id\n",
            "  shared_secret        = \"vpn-secret\"\n",
            "  ike_version          = \"2\"\n",
            "  local_traffic_selector_range = [\"10.0.0.0/8\"]\n",
            "  remote_traffic_selector_range = [\"10.128.0.0/24\"]\n",
            "}\n",
            "```\n",
            "User confirms this Terraform code? (yes/no): yes\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# Define the initial prompt template for generating project architecture\n",
        "architecture_prompt_template = \"\"\"Given the project description below, generate an initial architecture outline suitable for GCP using Terraform. Keep it concise and to the point.\n",
        "\n",
        "Project Description:\n",
        "{project_description}\n",
        "\n",
        "User Architecture Feedback:\n",
        "{user_architecture_feedback}\n",
        "\n",
        "Initial Architecture Outline:\"\"\"\n",
        "\n",
        "# Define the prompt template for generating Terraform code based on confirmed architecture\n",
        "terraform_prompt_template = \"\"\"\n",
        "Given the confirmed architecture outline below, generate the full Terraform code necessary to deploy the project on GCP. If you are asked for a revision, please make minimal changes to the existing Terraform code to accommodate the new request. Do not generate completely new code based on the asked revision. Try to find the most optimal way to add the new request while still maintaining the existing code structure.\n",
        "\n",
        "Confirmed Architecture Outline:\n",
        "{confirmed_architecture}\n",
        "\n",
        "Existing Terraform Code:\n",
        "{existing_terraform_code}\n",
        "\n",
        "User Feedback:\n",
        "{user_terraform_feedback}\n",
        "\n",
        "Revised Terraform Code:\"\"\"\n",
        "\n",
        "# Custom prompt templates\n",
        "architecture_prompt = PromptTemplate.from_template(architecture_prompt_template)\n",
        "terraform_code_prompt = PromptTemplate.from_template(terraform_prompt_template)\n",
        "\n",
        "# Define the interaction chain for architecture generation and confirmation\n",
        "architecture_chain = (\n",
        "    {\"project_description\": RunnablePassthrough(), \"user_architecture_feedback\": RunnablePassthrough()}\n",
        "    | architecture_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Define the interaction chain for Terraform code generation\n",
        "terraform_code_chain = (\n",
        "    {\"confirmed_architecture\": RunnablePassthrough(), \"existing_terraform_code\": RunnablePassthrough(), \"user_terraform_feedback\": RunnablePassthrough()}\n",
        "    | terraform_code_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Define the prompt template for suggesting improvements\n",
        "improvement_prompt_template = \"\"\"\n",
        "Given the confirmed architecture outline below, suggest potential improvements that could enhance the project's efficiency, scalability, or reliability.\n",
        "\n",
        "Confirmed Architecture Outline:\n",
        "{confirmed_architecture}\n",
        "\n",
        "Suggested Improvements:\"\"\"\n",
        "\n",
        "improvement_prompt = PromptTemplate.from_template(improvement_prompt_template)\n",
        "\n",
        "# Define the interaction chain for improvement suggestions\n",
        "improvement_chain = (\n",
        "    {\"confirmed_architecture\": RunnablePassthrough()}\n",
        "    | improvement_prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Get the project description from the user\n",
        "project_description = input(\"Enter the project description: \")\n",
        "\n",
        "user_architecture_feedback = \"\" # Initializing user feedback\n",
        "user_terraform_feedback = \"\" # Initializing user feedback\n",
        "existing_terraform_code = \"\" # Initializing existing Terraform code\n",
        "\n",
        "while True:\n",
        "    # Run the architecture_chain\n",
        "    architecture = architecture_chain.invoke({\"project_description\": project_description, \"user_architecture_feedback\": user_architecture_feedback})\n",
        "\n",
        "    # Display the generated architecture to the user\n",
        "    print(\"Generated Architecture: \", architecture)\n",
        "\n",
        "    # Ask for user confirmation\n",
        "    user_confirmation = input(\"User confirms this architecture? (yes/no): \")\n",
        "\n",
        "    # If user confirms, break the loop\n",
        "    if user_confirmation.lower() == 'yes':\n",
        "        break\n",
        "    else:\n",
        "        user_architecture_feedback = input(\"Please describe the issues you have with the architecture: \")\n",
        "\n",
        "# After user confirmation, run the improvement_chain\n",
        "improvements = improvement_chain.invoke({\"confirmed_architecture\": architecture})\n",
        "\n",
        "# Display the suggested improvements to the user\n",
        "print(\"Suggested Improvements: \", improvements)\n",
        "\n",
        "# Ask for user confirmation\n",
        "user_confirmation = input(\"Would you like to incorporate these improvements? (yes/no): \")\n",
        "\n",
        "# If user confirms, update the architecture\n",
        "if user_confirmation.lower() == 'yes':\n",
        "    architecture += \"\\n\\nIncorporated Improvements:\\n\" + improvements\n",
        "\n",
        "# After user confirmation, run the terraform_code_chain\n",
        "while True:\n",
        "    # Invoke the Terraform code generation chain\n",
        "    new_terraform_code = terraform_code_chain.invoke({\"confirmed_architecture\": architecture, \"existing_terraform_code\": existing_terraform_code, \"user_terraform_feedback\": user_terraform_feedback})\n",
        "\n",
        "    # Display the generated Terraform code\n",
        "    print(\"Generated Terraform Code: \", new_terraform_code)\n",
        "\n",
        "    # Ask for user confirmation\n",
        "    user_confirmation = input(\"User confirms this Terraform code? (yes/no): \")\n",
        "\n",
        "    # If user confirms, break the loop\n",
        "    if user_confirmation.lower() == 'yes':\n",
        "        existing_terraform_code = new_terraform_code  # Update existing Terraform code\n",
        "        break\n",
        "    else:\n",
        "        user_previous_feedback = user_terraform_feedback  # Store previous user feedback\n",
        "        user_terraform_feedback = input(\"Please describe the issues you have with the Terraform code: \")\n",
        "\n",
        "        # Incorporate both previous and new feedback into the architecture\n",
        "        architecture += \"\\n\\nUser Feedback from Previous Iteration:\\n\" + user_previous_feedback + \"\\n\"\n",
        "        architecture += \"\\n\\nUser Feedback from Current Iteration:\\n\" + user_terraform_feedback + \"\\n\"\n",
        "\n",
        "# Save the final Terraform code to a .hcl file\n",
        "with open('terraform_code.hcl', 'w') as file:\n",
        "    file.write(existing_terraform_code)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}